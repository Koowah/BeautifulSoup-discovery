{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup #parser - can be made to crawl through while True + code as shown below\n",
    "import requests #easilly send http requests to retrieve \n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://boston.craigslist.org/search/ofc\" #page we want to scrape and start crawling from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_soup(url):\n",
    "    response = requests.get(url)\n",
    "    data = response.text\n",
    "    return BeautifulSoup(data, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time to get soup is 0.7367708683013916 seconds\n",
      "time to find_all is 0.008003711700439453 seconds\n",
      "Job title : PT Jack of all trades\n",
      "Location : Hudson metro west\n",
      "Date : Dec 26\n",
      "Link : https://boston.craigslist.org/bmw/ofc/d/hudson-pt-jack-of-all-trades/7425013381.html\n",
      "Description : \n",
      "\n",
      "QR Code Link to This Post\n",
      "\n",
      "\n",
      "A small tax/business office in Hudson area that is in growing pains mode and is looking for PT Jack of All Trades.  We do all types of returns and deal with clients who have IRS tax problems as well. Prefer a candidate that can work multiple projects, maintaining individual client needs (hand holding) and is a good multi-tasker. We tend to attract people with issues, so cleaning up messes and saving clients from themselves is not uncommon. 10-20 hours (off season)/15+ hours (Feb-April) a week is needed with definite possibility for more in the future and flexible hours are available. We are a scanning office that uses a client portal and Document Management system. COVID: Because of the small size of my home office, you will need to share another desk with the other tax preparer. This will happen on alternate days; you will not be sharing the desk on the same day. Separate keyboard, mouse provided in addition to wipes and sanitizer. The office has a separate entrance from home. SIMPLE plan offered and Independent Contractors are encouraged to apply. \n",
      "Some of the duties are:\n",
      "•\tHelping with train wrecks. Sometimes we need to dig in the piles to find out what the client forgot to mention or has. \n",
      "•\tPossibly helping with basic tax returns\n",
      "•\tMaintain client contact info\n",
      "•\tBookkeeping for 2 estates and Airbnb\n",
      "•\tHelp schedule and organize an AirBnB\n",
      "•\tHelp organize office and provide new ways to make it run better, including updating the procedure manual.\n",
      "•\tPosting blogs and newsletters to word press website and social media \n",
      "•\tBeing nice to the clients but being stern with the owner when he doesn’t follow his own procedures. \n",
      "•\tIf the owner says he will do something, but you know you can do it better or the owner shouldn’t do it, you need to tell him you will do it and get it done.\n",
      "•\tIf there is a better way to do things, don’t keep it a secret \n",
      "\n",
      "Qualifications: \n",
      "\n",
      "•\tStrong proficiency w/ Excel, Word, and Outlook\n",
      "•\tDon’t be afraid of learning new software we end up adopting\n",
      "•\tKnowledge of Drake tax software, Pitbull Tax resolution software and QuickBooks is helpful. \n",
      "•\tSeveral years of bookkeeping/accounting/tax experience would be helpful as well as Public accounting/tax firm experience. \n",
      "•\tSelf-starter, strong work ethic, (take charge and dig in)\n",
      "•\tability to meet deadlines (don’t constantly ask me what needs to be done and finished)\n",
      "•\tAttention to detail needed with good communication and organizational skills\n",
      "•\tAble to handle a schedule,\n",
      "•\tGood sense of humor, especially when cleaning up the client’s mess and/or when the owner is sarcastic or occasionally swears\n",
      "•\tDon’t be afraid to tell the owner he is not following procedures\n",
      "•\tWould love you to know the ins & outs of social media\n",
      "\n",
      "\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    \n",
    "    soup = get_soup(url)\n",
    "    jobs = soup.find_all('div', {'class':'result-info'})\n",
    "\n",
    "    for job in jobs: #scrapes job offers in from page(url)\n",
    "\n",
    "        title = job.find('a',{'class':'result-title'}).text\n",
    "        location_tag = job.find('span', {'class':'result-hood'})\n",
    "        location = location_tag.text[2:-1].strip() if location_tag else job.find('span', {'class':'nearby'}).text\n",
    "        date = job.find('time', {'class':'result-date'}).text\n",
    "        link = job.find('a',{'class':'result-title'}).get('href')\n",
    "\n",
    "        description_response = requests.get(link)\n",
    "        description_soup = BeautifulSoup(description_response.text, 'html.parser')\n",
    "        description = description_soup.find('section', {'id':'postingbody'}).text\n",
    "\n",
    "        print(f'Job title : {title}\\nLocation : {location}\\nDate : {date}\\nLink : {link}\\nDescription : {description}\\n\\n\\n---\\n\\n\\n')\n",
    "    \n",
    "\n",
    "    next_page_tag = get_soup(url).find('a', {'class':'button next'}) # next page div\n",
    "\n",
    "    if next_page_tag.get('href'): # if there is a link, update url to crawl next page\n",
    "        url = 'https://boston.craigslist.org' + next_page_tag.get('href')\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***requests.get*** takes about .5 seconds - it is called **once per page** but more importantly **once per offer**\n",
    "\n",
    "***find*** takes virtually no time\n",
    "\n",
    "time to get job description takes .35 which is roughly equal to the request time (parse + find pretty quick) \n",
    "\n",
    "*We need to parallel process requesting the descriptions of each offer within a page*\n",
    "\n",
    "After that, we might parallel process each page's scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**multithread request descriptions <=> multithread for job in jobs loop**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best way seems to first get all the urls of a page in a list and follow https://beckernick.github.io/faster-web-scraping-python/"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7e2f77450d574f743a6d9d7e6faf87aa64c8f9c3d83f2013a355475f93227b48"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('data': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
